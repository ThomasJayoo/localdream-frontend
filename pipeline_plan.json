{
  "main_crawler": {
    "file": "crawl_all.py",
    "method": "requests + BeautifulSoup",
    "description": "ê¸°ë³¸ í¬ë¡¤ë§ ìˆ˜í–‰. selector_list_230_fixed.json ê¸°ë°˜ ê° ì§€ìì²´ ê³µì§€ì‚¬í•­ì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ.",
    "error_handling": [
      "try-exceptë¡œ ê°œë³„ ì§€ì—­ë³„ í¬ë¡¤ë§ ì‹¤íŒ¨ catch",
      "ë¡œê·¸ íŒŒì¼ì— ì‹¤íŒ¨ ì§€ì—­, ì˜¤ë¥˜ ë©”ì‹œì§€ ê¸°ë¡",
      "ë¹ˆ ë‰´ìŠ¤ í•­ëª© ìë™ skip"
    ],
    "output": "raw_news.json"
  },
  "gpt_classifier": {
    "step": "ë‚´ë¶€ ë˜ëŠ” í›„ì† ì²˜ë¦¬",
    "description": "GPTë¥¼ í˜¸ì¶œí•˜ì—¬ ë‰´ìŠ¤ ì œëª©ì„ 6ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ìë™ ë¶„ë¥˜",
    "fallback": "ë¶„ë¥˜ ì‹¤íŒ¨ ì‹œ 'ì»¤ë®¤ë‹ˆí‹°ë‰´ìŠ¤'ë¡œ ì§€ì •"
  },
  "news_exporter": {
    "output": "public/data/news.json",
    "description": "ì¹´í…Œê³ ë¦¬ë³„ ì •ë ¬ í›„ ìµœì‹  5ê°œ ë‰´ìŠ¤ë§Œ ì €ì¥. news.json í¬ë§·ìœ¼ë¡œ ë³€í™˜",
    "structure": {
      "ë³µì§€": [],
      "ì¶•ì œ": [],
      "ê´€ê´‘í™ë³´": [],
      "ê±´ì„¤í–‰ì •": [],
      "ì»¤ë®¤ë‹ˆí‹°ë‰´ìŠ¤": [],
      "ì¸êµ¬ëŒ€ì±…": []
    }
  },
  "backup_selenium": {
    "file": "selenium_notice_enhanced.py",
    "trigger": "crawl ì‹¤íŒ¨ ì§€ì—­ë§Œ ì¡°ê±´ë¶€ ì‹¤í–‰",
    "description": "requestsë¡œ ì‹¤íŒ¨í•œ íŠ¹ì • ì§€ì—­ë§Œ seleniumìœ¼ë¡œ ì¬ì‹œë„",
    "output_merge": "raw_news.jsonì— append í›„ dedup ì²˜ë¦¬"
  },
  "logging": {
    "fail_log": "logs/failed_regions.log",
    "run_log": "logs/crawl_log_20250602.log",
    "json_dump_on_error": true
  },
  "automation": {
    "bat_file": "generate_news_pipeline.bat",
    "steps": [
      "python backend/crawl_all.py",
      "python backend/gpt_classifier.py",
      "git add public/data/news.json",
      "git commit -m \"ğŸ”„ ë‰´ìŠ¤ ìë™ ì—…ë°ì´íŠ¸\"",
      "git push"
    ]
  }
}